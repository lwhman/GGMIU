import math

import dgl
import dgl.function as fn
import dgl.nn.pytorch as dglnn
import dgl.ops as F
import numpy as np
import torch as th
import torch.nn as nn
import torch.nn.functional as TF
from scipy import sparse

from src.models.gnn_models import GATConv
# from src.models import handle
from src.models.handle import GlobalAggregator


class SemanticExpander(nn.Module):
    
    def __init__(self, input_dim, reducer, order):
        
        super().__init__()
        
        self.input_dim = input_dim
        self.order = order
        self.reducer = reducer
        self.GRUs = nn.ModuleList()
        for i in range(self.order):
            self.GRUs.append(nn.GRU(self.input_dim, self.input_dim, 1, True, True))
    
        if self.reducer == 'concat':
            self.Ws = nn.ModuleList()
            for i in range(1, self.order):
                self.Ws.append(nn.Linear(self.input_dim * (i+1), self.input_dim))
    #初始化s1即为item的embedding, 对于更高level的intent unit，初始化采用两种方法相加的方式：一种是采用平均池化，最大池化等方法；一种是采用GRU来提取order-sensitive intent。
    def forward(self, feat):
        
        if len(feat.shape) < 3:#s1级别是长度小于3的
            return feat
        if self.reducer == 'mean':
            invar = th.mean(feat, dim=1)#s2,s3针对项目节点特征做mean操作，项目特征就是dim=1,维度1没了num_nodes*dim
        elif self.reducer == 'max':
            invar =  th.max(feat, dim=1)[0]
        elif self.reducer == 'concat':
            invar =  self.Ws[feat.size(1)-2](feat.view(feat.size(0), -1))
        var = self.GRUs[feat.size(1)-2](feat)[1].permute(1, 0, 2).squeeze()#输出维度：node_num*dim,GRU有order层，现在取feat.size(1)层，假设是2，2-2=0，就是第0层，然后输入feat到GRU层，输出有两个tensor，分别otput和h0，[1]去h0：numnodes*numlayer*dim

        # return invar + var
        return 0.5 * invar + 0.5 * var
      
class MSHGNN(nn.Module):
    
    def __init__(self, input_dim, output_dim, dropout=0.0, activation=None, order=1, reducer='mean'):
        super().__init__()
     
        self.dropout = nn.Dropout(dropout)
        # self.gru = nn.GRUCell(2 * input_dim, output_dim)
        self.output_dim = output_dim
        self.activation = activation
        self.order = order
        
        conv1_modules = {'intra'+str(i+1) : GATConv(input_dim, output_dim, 8, dropout, dropout, residual=True) for i in range(self.order)}#8是多头注意力，feat_drop，attn_drop
        conv1_modules.update({'inter'     : GATConv(input_dim, output_dim, 8, dropout, dropout, residual=True)})#update添加
        self.conv1 = dglnn.HeteroGraphConv(conv1_modules, aggregate='sum')
        
        conv2_modules = {'intra'+str(i+1) : GATConv(input_dim, output_dim, 8, dropout, dropout, residual=True) for i in range(self.order)}
        conv2_modules.update({'inter'     : GATConv(input_dim, output_dim, 8, dropout, dropout, residual=True)})
        self.conv2 = dglnn.HeteroGraphConv(conv2_modules, aggregate='sum')
        
        self.lint = nn.Linear(output_dim, 1, bias=False)
        self.linq = nn.Linear(output_dim, output_dim)
        self.link = nn.Linear(output_dim, output_dim, bias=False)
        
    def forward(self, g, feat):

        with g.local_scope():
            #h1和h2是聚合函数后的结果，聚合目标节点来自各种关系的消息
            h1 = self.conv1(g, (feat, feat))#公式3.hv=hv(->)：node_num*head*dim，这个是图注意力机制放入dgl的异构图进行传播的,第一个参数是异构图，第二个参数是input,输入节点特征，字典性
            h2 = self.conv2(g.reverse(copy_edata=True), (feat, feat))#公式3。hv=hv(->)：node_num*head*dim，图的边是反向边，实际conv2和conv1是一样的参数
            h = {}
            for i in range(self.order):#公式4部分，hv=hv(->)+hv(<-)
                hl, hr = th.zeros(1, self.output_dim).to(self.lint.weight.device), th.zeros(1, self.output_dim).to(self.lint.weight.device)#hl和hr表示节点的in-nei和out-nei,shape都是1*dim,1是因为这里是一个节点的特征进行
                if 's'+str(i+1) in h1:
                    hl = h1['s'+str(i+1)]#node_num*head*dim，in
                if 's'+str(i+1) in h2:
                    hr = h2['s'+str(i+1)]#node_num*head*dim，out
                h['s'+str(i+1)] = hl + hr#node_num*head*dim.由于图是有向图，每个意图单元都具有in和out邻居节点
                if len(h['s'+str(i+1)].shape) > 2:#3维度只能是多头注意力的情况
                    h['s'+str(i+1)] = h['s'+str(i+1)].max(1)[0]#node_num*dim.torch.max返回值和索引。[0]取值.max之后的维度少了[1]维度.奇怪，明明论文没有提到max
                h_mean = F.segment.segment_reduce(g.batch_num_nodes('s'+str(i+1)), feat['s'+str(i+1)], 'mean')#batch_size*dim segment_reduce就是将第一参数复制有第二参数[1:]列.g.batch_num_nodes返回的是一个整数列表且len(整数列表)=batchsize。segment_reduce返回的shape ``(len(seglen), value.shape[1:])``
                h_mean = dgl.broadcast_nodes(g, h_mean, ntype='s'+str(i+1)) # adding mean maskes better,这个也算readout一种，我不太懂，由维度batchsize*dim变成了node_num*dim
                # print(h['s'+str(i+1)].shape, h_mean.shape)
                h['s'+str(i+1)] =  h_mean + h['s'+str(i+1)]#hv=hv(sum)+hv(mean)
                
        return h
                
    
class AttnReadout(nn.Module):
    def __init__(
        self,
        input_dim,
        hidden_dim,
        output_dim,
        feat_drop=0.0,
        activation=None,
        order=1,
        device=th.device('cpu')
    ):
        super().__init__()
        self.feat_drop = nn.Dropout(feat_drop)
        self.order = order
        self.device = device
        self.fc_u = nn.ModuleList()
        self.fc_v = nn.ModuleList()
        self.fc_e = nn.ModuleList()
        self.fc_p = nn.ModuleList()
        for i in range(self.order):
            self.fc_u.append(nn.Linear(input_dim, hidden_dim, bias=True))
            self.fc_v.append(nn.Linear(input_dim, hidden_dim, bias=False))
            self.fc_e.append(nn.Linear(hidden_dim, 1, bias=False))
        self.fc_out = (
            nn.Linear(input_dim, output_dim, bias=False)
            if output_dim != input_dim
            else None
        )
        self.activation = activation
    #计算zkg，也就是rst
    def forward(self, g, feats, last_nodess):
        #feats为公式5的hc
        rsts = []
      
        nfeats = []
        for i in range(self.order): 
            feat = feats['s'+str(i+1)]#node_num*dim
            feat = th.split(feat, g.batch_num_nodes('s'+str(i+1)).tolist())#batch_size*（各个子图的node_num）*dim详细见nfeats..在feat上划分为多少个.默认为维度0.split后是元组，feat[0]表示0这个子图的所有节点embedding表示.g.batch_num_nodes返回的s1的结点数列表.假设s1有3个子图，那就饭后有[2,2,1]，其中s1总结点数5个，len([2,2,1])=3.那么如果进行了split划分后，一共生成了len块。这里的异构图每一级都是batchsize个图。所以最后是batchsize*dim
            feats['s'+str(i+1)] = th.cat(feat, dim=0)#node_num*dim.
            nfeats.append(feat)#nfeats存储的是所有级别各个子图的embedding表示。维度：order*batchsie*(各个子图的node_num)*dim.也就是说nfeats[0][0]就是第1级别第0个子图的所有节点embedding.feats[0][0][0]就是第1级别第0个子图的第0个节点的节点embedding.
        feat_vs= th.cat(tuple(feats['s'+str(i+1)][last_nodess[i]].unsqueeze(1) for i in range(self.order)), dim=1)#维度：batchsize*order*dim.feat_vs为feats中s级的last_nodeid相应的位置值，last_node维度batchsize.unsquese后每一级是batchsize*1*dim。cat之后batch*order*dim
        feats = th.cat([th.cat(tuple(nfeats[j][i] for j in range(self.order)), dim=0) for i in range(len(g.batch_num_nodes('s1')))], dim=0)#维度：（s1.node_num+s2.node_num+s3.node_num）*dim.外循环batch次，内循拼接的是不同级别的第i个子图.也就是说每一次batch循环，拼接的是不同级别的子图，如batch1，2..batch,拼接的是[s1的1子图节点，s2的1子图节点，s3的1子图节点，s1的2子图节点，s2的2子图节点，s3的2子图节点，...s1的batch子图节点，s2的batch子图节点，s3的batch子图节点]
        batch_num_nodes = th.cat(tuple(g.batch_num_nodes('s'+str(i+1)).unsqueeze(1) for i in range(self.order)), dim=1).sum(1)#大小：batchsize个。batch_num_nodes[0]表示的是s1,s2,s3的第一个子图的结点数之和。这样看和feats是相对应的，feats[batch_num_nodes[0]]就是对应的特征表示
       #feat_u每次循环输入的是所有s的节点特征，但是每次循环都是不一样的，feat_v每次循环输入的是相应的第si个label
        idx = th.cat(tuple(th.ones(batch_num_nodes[j])*j for j in range(len(batch_num_nodes)))).long()#大小：（s1.node_num+s2.node_num+s3.node_num）个.创建batch_num_nodes的节点索引。比如batch_num_nodes[3]=2,则torch.ones得到的是[1,1]，乘上3后有[3,3].这样就知道是batch_num_nodes下标为3的内容。且batch_num_nodes[3]表示s1,s2,s3第4个子图的结点数之和
        for i in range(self.order):
            feat_u = self.fc_u[i](feats) #公式6的hc.线性模型,维度:(s1.node_num+s2.node_num+s3.node_num)*dim.
            feat_v = self.fc_v[i](feat_vs[:, i])[idx]#公式6的zkl.线性模型,bias=false.维度:(s1.node_num+s2.node_num+s3.node_num)*dim。返回的内容格式标签后的线性变换。对应feat_u。feat_vs[:, i]是因为feat_vs维度是batchsize*order*dim..feat_vs[:, i]输入的大小是batchsize*dim。所以self.fc_v输出的维度是batchsize*dim。又因为[idx]一共有(s1.node_num+s2.node_num+s3.node_num)大小。且idx的值在0~batch-1范围波动。所以最后是维度。
            e = self.fc_e[i](th.sigmoid(feat_u + feat_v))#公式6的ykc.线性模型，out_dim=1 维度(s1.node_num+s2.node_num+s3.node_num)*1
            alpha = F.segment.segment_softmax(batch_num_nodes, e)#公式5的softmax值。做分段激活函数，返回值形状和e一样大小。
            
            feat_norm = feats * alpha#公式5相乘.维度：（s1.node_num+s2.node_num+s3.node_num）*dim
            rst = F.segment.segment_reduce(batch_num_nodes, feat_norm, 'sum')#公式5的sum:c1+c2+。。。维度：[batchsize*dim]=[batch_num_nodes[0],feat_norm[1]]
            rsts.append(rst.unsqueeze(1))
        
            if self.fc_out is not None:
                rst = self.fc_out(rst)
            if self.activation is not None:
                rst = self.activation(rst)
        rst = th.cat(rsts, dim=1)#因为rsts有order个列表，每个列表都是batchsize*1*dim.因此拼接后维度就是batchsize*order*dim
        
        return rst#返回global的值，也就是zkg

class MSGIFSR(nn.Module):
    def __init__(self, adj_all, num, args, num_items, datasets, embedding_dim, num_layers, dropout=0.0, reducer='mean', order=3, norm=True, extra=True, fusion=True, device=th.device('cpu')):
    # def __init__(self, num_items, datasets, embedding_dim, num_layers, dropout=0.0, reducer='mean', order=3, norm=True, extra=True, fusion=True, device=th.device('cpu')):
        super().__init__()
        
        self.embeddings = nn.Embedding(num_items, embedding_dim, max_norm=1)
 
        self.num_items = num_items
        self.register_buffer('indices', th.arange(num_items, dtype=th.long))
        self.embedding_dim = embedding_dim
        self.num_layers = num_layers
        self.layers   = nn.ModuleList()
        input_dim     = embedding_dim
        self.reducer  = reducer
        self.order    = order
        self.alpha    = nn.Parameter(th.Tensor(self.order))
        self.beta     = nn.Parameter(th.Tensor(1))
        self.norm     = norm
        self.expander = SemanticExpander(input_dim, reducer, order)

        '''
        GCE参数
        '''
        self.num = th.Tensor(num).to(device)
        self.args = args
        self.dim = args.embedding_dim
        self.hop = args.n_iter
        self.sample_num = args.n_sample
        self.adj_all = th.Tensor(adj_all).to(device)
        self.adj_all = self.adj_all.long()
        self.dropout_global = args.dropout_global
        self.dropout_srl = args.dropout_srl

        self.global_agg = []
        for i in range(self.hop):
            if args.activate == 'relu':
                agg = GlobalAggregator(self.dim, args.dropout_gcn, act=th.relu)
            else:
                agg = GlobalAggregator(self.dim, args.dropout_gcn, act=th.tanh)
            self.add_module('agg_gcn_{}'.format(i), agg)
            self.global_agg.append(agg)
        #
        
        '''
        target attention，改
        '''
        self.linear_t = nn.Linear(self.dim, self.dim, bias=False)

        self.dropout_hg_l = args.dropout_hg_l
        #
        
        self.device = device
        for i in range(num_layers):
            layer = MSHGNN(
                input_dim,
                embedding_dim,
                dropout=dropout,
                order=self.order,
                activation=nn.PReLU(embedding_dim)
            )
            self.layers.append(layer)
            
        self.readout = AttnReadout(
            input_dim,
            embedding_dim,
            embedding_dim,
            feat_drop=dropout,
            activation=None,
            order=self.order,
            device=self.device
        )
        input_dim += embedding_dim
        self.feat_drop = nn.Dropout(dropout)

        self.fc_sr = nn.ModuleList()
        for i in range(self.order):#GCE:因为我多加了一层hg_g
            self.fc_sr.append(nn.Linear((input_dim+embedding_dim), embedding_dim, bias=False))
        
        self.sc_sr = nn.ModuleList()
        for i in range(self.order):
            self.sc_sr.append(nn.Sequential(nn.Linear(embedding_dim, embedding_dim, bias=True),  nn.ReLU(), nn.Linear(embedding_dim, 2, bias=False), nn.Softmax(dim=-1)))
        self.input_dim = input_dim
        self.embedding_dim =embedding_dim
 
        # self.sr_trans1 = nn.Linear(embedding_dim, embedding_dim)
        # self.sr_trans2 = nn.Linear(embedding_dim, embedding_dim)
        self.reset_parameters()
        self.alpha.data = th.zeros(self.order)
        self.alpha.data[0] = th.tensor(1.0)
        # self.beta.data = th.zeros(1)
        self.beta.data = th.tensor(1.0)
        self.fusion = fusion
        self.extra = extra
        self.epoch = 0



        
    def inc_epoch(self):
        self.epoch += 1
          
    def reset_parameters(self):
        stdv = 1 / math.sqrt(self.embedding_dim)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, stdv)

    #添加gce参数
    def sample(self, target, n_sample):
        # neighbor = self.adj_all[target.view(-1)]
        # index = np.arange(neighbor.shape[1])
        # np.random.shuffle(index)
        #n_sample表示取前n个邻居样本
        # index = index[:n_sample]
        # return self.adj_all[target.view(-1)][:, index], self.num[target.view(-1)][:, index]
        #这个return主要返回的是target这些项目在adj_all的第几行，也就是取target项目的邻居项目。self.num返回的是target项目邻居项目的次数
        #返回的维度是target.view(-1)行，adj_all[1].shape列。
        return self.adj_all[target.view(-1)], self.num[target.view(-1)]
            
    def iid2rp(self, iid):
        tmp = th.sum(th.cat(tuple(th.unique(tmp, return_inverse=True)[1].unsqueeze(0) for tmp in th.unbind(iid, dim=0)), dim=0), dim=1)
        
        return tmp
        
    def residual(self, h1, res):
        
        for key in h1.keys():
            h1[key] += res[key]
        
        return h1
        
    def forward(self, mg,inputs, mask_item, item):

        #gce内容
        mask_item = th.cat([mask_item[idx].unsqueeze(0) for idx in range(len(mask_item))], 0)
        item = th.cat([item[idx].unsqueeze(0) for idx in range(len(item))], 0)
        inputs = th.cat([inputs[idx].unsqueeze(0) for idx in range(len(inputs))], 0)
        batch_size = inputs.shape[0]
        seqs_len   = inputs.shape[1]
        # global
        # global-based graph更新
        # item_neighbors=batsize*len，此时[inputs]因为套了一个[],因此item_neighbors是一个列表。item_neighbors[0]就是input本身
        item_neighbors = [inputs]  # #inputs是unique去重的会话然后补0
        weight_neighbors = []
        support_size = seqs_len

        # self.hop是迭代次数，以1次为例（代码默认值为1）。做多少次在adj_all取出属于item_neighbors为开头的邻居项目、次数的提取
        # 取出会话包含的item的所有邻居及其出现次数记为item_sample_i,weight_sample_i
        for i in range(1, self.hop + 1):
            # self.sample表示在item_neighbors[-1]数据集上取self.sample_num个样本
            # item_sample_i,weight_sample_i的大小是2维度。(batchsize*len)*sample_num
            # item_sample_i是在adj_all取出属于item_neighbors为开头的邻居项目。item_neighbors初始化多了一个[],因此item_neighbors[-1]表示inputs这个玩意
            item_sample_i, weight_sample_i = self.sample(item_neighbors[-1], self.sample_num)
            # 这里的做法是因为下面item_sample_i要重新变成维度batsize*len
            support_size *= self.sample_num
            # item_neighbor[i+1]是item_neighbor[i]的邻居项目
            # item_neighbors是列表，当hop=2是有3个数组。[原始输入项目，项目的邻居，项目邻居的邻居]
            item_neighbors.append(item_sample_i.view(batch_size, support_size))
            weight_neighbors.append(
                weight_sample_i.view(batch_size, support_size))  # weight_neighbors最后是2个[原始输入项目邻居次数，项目的邻居次数]

        # 这样item_neighbors就包含两项，一项是batch的会话序列，一项是这些会话序列对应的邻居，取出它们的embedding
        # entity_vectors就是item_neighbors的embedding,以前我以为embedding的输入是指一个batchsize输入的总数，原来是整个数据集的项目总数。这样才能保证每个项目都有各自的embedding。因此embedding的每次输入项目数量不限制
        # entity_vectors：batchsize*（len*sample_num）*dim，entity_vectors是去重的会话项目embedding
        entity_vectors = [self.embeddings(i) for i in item_neighbors]
        weight_vectors = weight_neighbors

        # 进行掩码操作，使得item_emb中补0的值对应的embedding也全为0
        session_info = []
        # item_emb维度：batchsize*len*dim
        # mask_item.float().unsqueeze(-1)是这样看的。因为item是embedding后，第2维度是dim（假设维度是012）.而mask_item原本是batchsize*len.没有第三维度dim。
        # item进行embedding后，有的会话它的项目是0，因此要mask_item进行掩码，不得不说广播好有用，mask_item虽然是batchsize*len,但是unsqueeze就是batchsize*len*1,乘法具有广播机制，自动帮mask_item复制
        item_emb = self.embeddings(item) * mask_item.float().unsqueeze(-1)  # item是原会话等长（补0等长的）

        # mean
        # torch.sum(mask_item.float(), -1).unsqueeze(-1)这样看的。
        # 因为这里是torch.sum在len个样本上做，也就是item_emb的第1维度。那么sum宛以后，第1维度没有了，就是item_emb【batchsize,dim】。
        # 同理，mask_item的len在第1维度，因此sum完以后只有[batchsize]，而要做除法保证维度相同，因此最后一个维度扩充
        # 所以sum_item_emb最后维度是【batchsize,dim】，sum_item_emb就是会话的item进行求和
        sum_item_emb = th.sum(item_emb, 1) / th.sum(mask_item.float(), -1).unsqueeze(
            -1)  # 公式3），当前会话项目的embedding进行求和

        # sum
        # sum_item_emb = torch.sum(item_emb, 1)

        # 因为sum_item_emb是2维度【batchsize,dim】。为了一下方便计算在第1维度【-2】做扩充
        sum_item_emb = sum_item_emb.unsqueeze(-2)
        for i in range(self.hop):
            # session_info存放的是global_agg图中，循环所要的所有的项目
            session_info.append(sum_item_emb.repeat(1, entity_vectors[i].shape[1], 1))

    
        # 是不断通过aggregator聚合函数得到新的glabol会话item的信息。下面的循环可以理解为[inuputs,inputs的邻居item,inputs的邻居item的邻居item],第一次循环[inuputs,inputs的邻居item]=h1,[inputs的邻居item，inputs的邻居item的邻居item]=h2,第二次循环[h1,h2]=h3.最后得到最后的结果entity_vectors=h3
        for n_hop in range(self.hop):
            entity_vectors_next_iter = []
            # 为下面聚合函数entity_vectors的运算。entity_vectors原本的维度是【batchsize,len*sample_num*dim】
            shape = [batch_size, -1, self.sample_num, self.dim]
            for hop in range(self.hop - n_hop):
                # 提取hop对应的聚合函数self.global_agg[n_hop]
                aggregator = self.global_agg[n_hop]
                # 具体实现在下一部分,#vector:batchsize * X *dim，vector得到这组会话项目item聚合邻居节点信息后的更新状态
                vector = aggregator(self_vectors=entity_vectors[hop],
                                    neighbor_vector=entity_vectors[hop + 1].view(shape),
                                    masks=None,
                                    batch_size=batch_size,
                                    neighbor_weight=weight_vectors[hop].view(batch_size, -1, self.sample_num),
                                    extra_vector=session_info[hop])
                # 将当前hop学习到的表示保存下来，作为下一个hop的输入
                entity_vectors_next_iter.append(vector)
            entity_vectors = entity_vectors_next_iter

        # 取出最后一次更新得到的表示（因为entity_vectors中只包含最后一次更新的表示，因此下标是0）
        # entity_vectors[0]是因为最后一次循环是，entity_vectors[0]表示项目，。entity_vectors长这样[[xxx]]
        # h_global:batchsize * len *dim
        h_global = entity_vectors[0].view(batch_size, seqs_len, self.dim)
        h_global = TF.dropout(h_global, self.dropout_global, training=self.training)
        #打算和srl一样，最后维度变成batchsize*order*dim。现在我打算先将len的维度进行处理
        mask = mask_item.float().unsqueeze(-1)
        hg_g = th.sum(h_global * mask, -2) / th.sum(mask, 1)#维度：batchsize*dim。通过计算会话的项目表示的平均值来获得会话信息，公式12
        hg_g = hg_g.unsqueeze(-2).repeat(1, self.order, 1)#维度：batchsize*order*dim
        #
        
        feats = {} #对所有级别的项目特征进行归一化存储
        for i in range(self.order):#order从0开始，对所有级别的项目特征进行归一化
            iid = mg.nodes['s' + str(i+1)].data['iid'] #第一次迭代，将s1的所有节点属性赋值到iid.节点属性就是会话的1级项目。同理，s2的节点属性是2个一起的
            iid = iid.type(th.LongTensor)
            iid=iid.to(self.device)
            # idd = th.Tensor(iid).long().to(self.device)
            # print('idd_type:',iid)
            feat = self.embeddings(iid) #生成节点的embedding。s1级别的num_nodes(iid长度)*embedding_dim，s2级别num_nodes(iid长度)*2*embedding_dim（从这里报错expected scalar type Long but found Int）
            feat = self.feat_drop(feat)#丢弃一些神经元
            feat = self.expander(feat)#    #初始化s1即为item的embedding, 对于更高level的intent unit，初始化采用两种方法相加的方式：一种是采用平均池化，最大池化等方法；一种是采用GRU来提取order-sensitive intent。
            if th.isnan(feat).any():
                feat = feat.masked_fill(feat != feat, 0) #masked_fill作为mask时序，就是特征有nan，则补0
            if self.norm:
                feat = nn.functional.normalize(feat, dim=-1) #则归一化
            feats['s' + str(i+1)] = feat
       
        h = feats #所有级别的汇总，每个级别都是nodes_num*dim
        for idx, layer in enumerate(self.layers):#misgifsr的层数，默认是1
            h = layer(mg, h)

        last_nodes = []
        for i in range(self.order):
            if self.norm:
                h['s'+str(i+1)] = nn.functional.normalize(h['s'+str(i+1)], dim=-1)
            last_nodes.append(mg.filter_nodes(lambda nodes: nodes.data['last'] == 1, ntype='s'+str(i+1)))#维度：512.返回的是label的数值。感觉就是找出满足s的条件函数的一维度张量。等于1是因为找到了last当label为1的位置
            
        feat = h#维度：order*node_num*dim
        sr_g = self.readout(mg, feat, last_nodes)   #sr_g维度：batchsize*order*dim ,feat维度：order*node_num*dim  ，last_nodes维度：order*dim

        sr_l = th.cat([feat['s'+str(i+1)][last_nodes[i]].unsqueeze(1) for i in range(self.order)], dim=1)#维度：batchsize*order*dim
        sr_l = TF.dropout(sr_l, self.dropout_srl , training=self.training)

        '''开始target attention'''
        #增加hg_l
        # hid = self.embeddings(item)#batchsize*len*dim
        # t = hid * mask_item.view(mask_item.shape[0], -1, 1).float()#t的维度：#batchsize*len*dim
        # qt = self.linear_t(t)#qt的维度：#batchsize*len*dim

        #变动2
        qt = self.linear_t(sr_g)  # qt的维度：#batchsize*len*dim

        b=self.embeddings(self.indices)[1:] #目标值的embedding
        beta = TF.softmax(b @ qt.transpose(1, 2), -1)# batch_size x （n_nodes-1） x seq_length
        # hg_l = beta @ hid # hg_l维度：batch_size x （n_nodes-1） x latent_size

        #变动2
        hg_l = beta @ sr_g  # hg_l维度：batch_size x （n_nodes-1） x latent_size

        hg_l = hg_l.sum(1).unsqueeze(1).repeat(1, self.order, 1) ## hg_l维度：batch_size x order x
        hg_l = TF.dropout(hg_l, self.dropout_hg_l, training=self.training)

        ''''''
        
        
        
        sr   = th.cat([sr_l, sr_g, hg_g], dim=-1)# .view(sr_l.size(0), -1)公式7#维度：batchsize*order*2dim.加入h_g应该就是batchsize*order*3dim
        sr   = th.cat([self.fc_sr[i](sr_i).unsqueeze(1) for i, sr_i in enumerate(th.unbind(sr, dim=1))], dim=1)#结果维度：batchsize*order*dim.unbind就是将我们的input从dim进行切片，并返回切片的结果，返回的结果里面没有dim这个维度.fc_sr相当于公式7的W3
        if self.norm:
            sr = nn.functional.normalize(sr, dim=-1)

        
        target = self.embeddings(self.indices)#num_items的总数。维度：num_items*dim,但是又因为gce内容里面，num_node是多了1个节点0，所以我要取[1:]
        #但是又因为gce内容里面，num_node是多了1个节点0，所以我要取[1:]
        target = target[1:]

        if self.norm:
            target = nn.functional.normalize(target, dim=-1)
               
        if self.extra:#是否whether use REnorm.
            logits = sr @ target.t()
            phi = self.sc_sr[0](sr).unsqueeze(-1)
            mask = th.zeros(phi.size(0), self.num_items).to(self.device)
            iids = th.split(mg.nodes['s1'].data['iid'], mg.batch_num_nodes('s1').tolist())
            for i in range(len(mask)):
                iid = iids[i].type(th.LongTensor)#报错
                mask[i, iid] = 1
                
            #报错，去掉第0维度
            mask_i = mask[0][1:].unsqueeze(0)
            for i in mask[1:]:
                mask_i = th.cat((mask_i, (i[1:].unsqueeze(0))), 0)

            logits_in = logits.masked_fill(~mask_i.bool().unsqueeze(1), float('-inf'))
            logits_ex = logits.masked_fill(mask_i.bool().unsqueeze(1), float('-inf'))
            score     = th.softmax(12 * logits_in.squeeze(), dim=-1)
            score_ex  = th.softmax(12 * logits_ex.squeeze(), dim=-1) 
          
            if th.isnan(score).any():
                score    = feat.masked_fill(score != score, 0)
            if th.isnan(score_ex).any():
                score_ex = score_ex.masked_fill(score_ex != score_ex, 0)
            assert not th.isnan(score).any()
            assert not th.isnan(score_ex).any()
            # print(score.shape, score_ex.shape)
            if self.order == 1:
                phi = phi.squeeze(1)
                score = (th.cat((score.unsqueeze(1), score_ex.unsqueeze(1)), dim=1) * phi).sum(1)
            else:
                score = (th.cat((score.unsqueeze(2), score_ex.unsqueeze(2)), dim=2) * phi).sum(2)
        else:
            # print("no extra ****************")
            logits = sr.squeeze() @ target.t()#公式8.维度：batchsize*order*num_items
            score  = th.softmax(12 * logits, dim=-1)
        
        if self.order > 1 and self.fusion:#self.fusion:whether use IFR
            alpha = th.softmax(self.alpha.unsqueeze(0), dim=-1).view(1, self.alpha.size(0), 1)
            g = th.ones(score.size(0), score.size(1), 1).to(self.device)
            g = alpha.repeat(score.size(0), 1, 1)
            score = (score * g).sum(1)
        elif self.order > 1:
            score = score[:, 0]
            
        # print(score.shape)
            
        score = th.log(score)
        
        return score
        
        
